---
title: "EDS 222: Homework 1"
date: Due 10/14
author: Bailey Jorgensen
editor_options: 
  chunk_output_type: inline
---

## Background

*(The case study in this exercise is based on reality, but does not include actual observational data.)*

In this exercise we will look at a case study concerning air quality in South Asia. The World Health Organization estimates that air pollution kills an estimated seven million people per year, due to its effects on the cardiovascular and respiratory systems. Out of the 40 most polluted cities in the world, South Asia is home to 37, and Pakistan was ranked to contain the second most air pollution in the world in 2020 (IQAIR, 2020). In 2019, Lahore, Pakistan was the 12th most polluted city in the world, exposing a population of 11.1 million people to increased mortality and morbidity risks.

In this exercise, you are given two datasets from Lahore, Pakistan and are asked to compare the two different data collection strategies from this city. These data are:

-   Crowd-sourced data from air quality monitors located in people's homes. These data are voluntarily collected by individual households who choose to install a monitor in their home and upload their data for public access.

-   Official government data from monitors installed by government officials at selected locations across Lahore. There have been reports that government officials strategically locate monitors in locations with cleaner air in order to mitigate domestic and international pressure to clean up the air.

::: callout-note
All data for EDS 222 will be stored on the Taylor server, in the shared `/courses/eds-222/data/` directory. Please see material from EDS 214 on how to access and retrieve data from Taylor. These data are small; all compute can be handled locally. Thanks to Bren PhD student Fatiq Nadeem for assembling these data!
:::

In answering the following questions, please consider the lecture content from class on sampling strategies, as well as the material in Chapter 2 of [*Introduction to Modern Statistics*](https://openintro-ims.netlify.app/data-design). Include in your submission your version of this file "`eds-222-hw1.qmd`" and the rendered HTML output, each containing complete answers to all questions *as well as the associated code*. Questions with answers unsupported by the code will be marked incomplete. Showing your work this way will help you develop the habit of creating reproducible code.

## Assessment

### Question 1

Load the data from each source and label it as `crowdsourced` and `govt` accordingly. For example:

``` r
crowdsourced <- readRDS(file.path("data", "airpol-PK-crowdsourced.RDS"))
govt <- readRDS(file.path("data", "airpol-PK-govt.RDS"))
```

```{r, output = FALSE}
# Load libraries
library(here)
library(tidyverse)
```

```{r, output = FALSE}
# Read in files and name them as instructed
crowdsourced <- readRDS(file.path("data", "airpol-PK-crowdsourced.RDS"))
govt <- readRDS(file.path("data", "airpol-PK-govt.RDS"))
```

::: callout-warning
There's an implicit assumption about file organization in the code above. What is it? How can you make the code work?

\# A similar filepath structure works with the here library installed. I've never seen the \`file.path\` function used before, however. Nor I have I used an RDS file before.
:::

1.  These dataframes have one row per pollution observation. How many pollution records are in each dataset?

```{r}
# This show that there are 5488 rows and four columns in the crowdsourced dataframe, meaning there are 5488 pollution observations

dim(crowdsourced)

# This shows that there are 1960 rows and 4 columns in the govt dataframe, meaning that there are 1960 pollution observations
dim(govt)
```

2.  Each monitor is located at a unique latitude and longitude location. How many unique monitors are in each dataset?

```{r}
# This asks for the unique values of both the latitude and longitude together by indexing into the dataframe, and then calling two column names

unique(crowdsourced[c("longitude", "latitude")])
```

::: callout-tip
`group_by(longitude,latitude)` and `cur_group_id()` in `dplyr` will help in creating a unique identifier for each (longitude, latitude) pair.

NOTE: I saw the group_by() and cur_group_id() homework hints after I had already made the above code. So, I kept it but played around with these two functions too. Below is what I came up with.

Both sets of code show that there are 14 unique lat and long combinations, so 14 different monitors. Is one set of code better or more accurate than the other?

```{r}
unique_monitors_cs <- crowdsourced |>
  group_by(latitude, longitude) |>
  mutate(unique_id = cur_group_id())

length(unique(unique_monitors_cs[["unique_id"]]))
                        
```
:::

```{r}
# Now, I will do the same code for the government dataframe

unique(govt[c("longitude", "latitude")])
```

```{r}
unique_monitors_govt <- govt |>
  group_by(latitude, longitude) |>
  mutate(unique_govt_id = cur_group_id())

length(unique(unique_monitors_govt[["unique_govt_id"]]))
```

### Question 2

The goal of pollution monitoring in Lahore is to measure the average pollution conditions across the city.

1.  What is the *population* in this setting? Please be precise.

**The population could be thought of as the actual amount of PM in the air of the city from November 4th 2018 to November 30th 2019.**

2.  What are the *samples* in this setting? Please be precise.

**The samples in this setting are the air quality measurements collected from both crowdsourced and government sourced air monitors from November 4th 2018 and November 30th 2019.**

3.  These samples were not randomly collected from across locations in Lahore. Given the sampling approaches described above, discuss possible biases that may enter when we use these samples to construct estimates of population parameters.

**Though the goal of the crowd-sourced data is to compare and try to find bias in the government data, the opposite could also be true, where citizens place sensors in particularly dirty air areas, leading to bias in the opposite direction. In addition, the sample bias could be introduced in both sets based on the people who are willing to have sensors placed in their homes. Either way, when constructing estimates of population parameters, these biases should be kept in mind.**

### Question 3

1.  For both the government data and the crowd-sourced data, report the sample mean, sample minimum, and sample maximum value of PM 2.5 (measured in $\mu g/m^3$).

```{r}

crowdsourced_mean <- mean(crowdsourced$PM)
print(crowdsourced_mean)

govt_mean <- mean(govt$PM)
print(govt_mean)

```

1.  Discuss any key differences that you see between these two samples.

**The mean air pollution concentration measured in** $\mu g/m^3$ is much higher in the crowd-sourced data versus the government collected data.

2.  Are the differences in mean pollution as expected, given what we know about the sampling strategies?

**This could be expected, based on the bias likely introduced into both samples given who was collecting them and for what purposes. Biases could arise in the crowd sourced data due to the type of people willing to participate as well as skill in using monitors. The government data could be biased because the monitors could be placed in certain areas in order to make the air quality look better than it actually is.**

### Question 4

Use the location of the air pollution stations for both of the sampling strategies to generate a map showing locations of each observation. Color the two samples with different colors to highlight how each sample obtains measurements from different parts of the city.

::: callout-tip
`longitude` indicates location in the *x*-direction, while `latitude` indicates location in the *y*-direction. With `ggplot2` this should be nothing fancy. We'll do more spatial data in `R` later in the course.
:::

```{r}
# First I will add a column for each dataset that indicates if it is govt or crowdsourced
crowdsourced$source <- ("crowdsourced")
govt$source <- ("govt")

# Then I will join the two datasets
all_data <- full_join(crowdsourced, govt)
```

```{r}
# Then I will create a plot indicating location, with points colored by source of dat
ggplot(data = all_data,aes(x = longitude, y = latitude, color = source)) +
  geom_point() +
  theme_bw()
```

### Question 5

The local newspaper in Pakistan, *Dawn*, claims that the government is misreporting the air pollution levels in Lahore. Do the locations of monitors in question 4, relative to crowd-sourced monitors, suggest anything about a possible political bias?

**According to our plot, it does seem that the government sensors are clustered in one location, while the crowd-sourced sensors are more widely and evenly distributed. This seems to indicate that the government samples have more location bias.**

### Question 6

Given the recent corruption in air quality reporting, the Prime Minister of Pakistan has hired an independent body of environmental data scientists to create an unbiased estimate of the mean PM 2.5 across Lahore using some combination of both government stations and crowd sourced observations.

NASA's satellite data indicates that the average PM across Lahore is 89.2 $\mu g/m^3$. Since this is the most objective estimate of population-level PM 2.5 available, your goal is to match this mean as closely as possible by creating a new ground-level monitoring sample that draws on both the government and crowd-sourced samples.

#### Question 6.1

First, generate a *random sample* of size $n=1000$ air pollution records by (i) pooling observations across the government and the crowd-sourced data; and (ii) drawing observations at random from this pooled sample.

```{r}
# To do this, I believe I can use the full join of the datasets that I already made earlier
# The below code will generate a random sample, sampling from the rows specifically, with no repeats

set.seed(4321)
random_sample <- all_data[sample(nrow(all_data), 1000, replace = FALSE), ]

kableExtra::kable(random_sample |>
                    group_by(source) |>
                    count())
```

::: callout-tip
`bind_rows()` may be helpful.
:::

Second, create a *stratified random sample*. Do so by (i) stratifying your pooled data-set into strata of 0.01 degrees of latitude, and (ii) randomly sampling 200 air pollution observations from each stratum.

```{r}
stratified <- random_sample %>%
  mutate(rounded = round(latitude, 2)) %>%  
  group_by(rounded) %>%
  slice_sample(n = 200) 

```

#### Question 6.2

Compare estimated means of PM 2.5 for each sampling strategy to the NASA estimate of 89.2 $\mu g/m^3$. Which sample seems to match the satellite data best? What would you recommend the Prime Minister do? Does your proposed sampling strategy rely more on government or on crowd-sourced data? Why might that be the case?

```{r}
mean(stratified$PM)
mean(random_sample$PM)
mean(govt$PM)
mean(crowdsourced$PM)
```

The means of the PM for each sampling strategy utilizing both the government and the crowd-sourced data are lower than the NASA data, as well as the original crowd-sourced data. The stratified sampling strategy is the closest to the NASA estimate. I would advise that the Prime Minister rely more on the stratified data, which depends more on the crowd-sourced data since that data spans more latitudes. The stratified sample covers the latitudes of the city more evenly than the random sample. The data source for the stratified sampling dataset samples every .01 degrees of latitude.
